---
pubDate: 2025-12-25
---

##### [DEW - The Year in Review 2025 by Ananth Packkildurai](https://www.dataengineeringweekly.com/p/dew-the-year-in-review-2025)
##### [Prompt caching: 10x cheaper LLM tokens, but how? | ngrok blog by ngrok](https://ngrok.com/blog/prompt-caching/)
##### [Zero to One: Learning Agentic Patterns by Philipp Schmid](https://www.philschmid.de/agentic-pattern)

- An initial LLM acts as a router, classifying the user's input and directing it to the most appropriate specialized task or LLM.
- Reflection Pattern

An agent evaluates its own output and uses that feedback to refine its response iteratively. This pattern is also known as Evaluator-Optimizer and uses a self-correction loop
- The key to success with any LLM application, especially complex agentic systems, is empirical evaluation. Define metrics, measure performance, identify bottlenecks or failure points, and iterate on your design. Resist to over-engineer

##### [How to Choose the Right Embedding Model for RAG - Milvus Blog](https://milvus.io/blog/how-to-choose-the-right-embedding-model-for-rag.md)

- Sparse vectors (like BM25) focus on keyword frequency and document length. They’re great for explicit matches but blind to synonyms and context—“AI” and “artificial intelligence” would look unrelated
- Dense vectors (like those produced by BERT) capture deeper semantics. They can see that “Apple releases new phone” is related to “iPhone product launch,” even without shared keywords. The downside is higher computational cost and less interpretability
- Since one token is roughly 0.75 words
- The key is balance. For most general-purpose applications, 768–1,536 dimensions strike the right mix of efficiency and accuracy. For tasks that demand high precision—such as academic or scientific searches—going beyond 2,000 dimensions can be worthwhile. On the other hand, resource-constrained systems (such as edge deployments) may use 512 dimensions effectively, provided retrieval quality is validated. In some lightweight recommendation or personalization systems, even smaller dimensions may be enough
- Under the hood, BERT’s input vectors combined three elements: token embeddings (the word itself), segment embeddings (which sentence it belongs to), and position embeddings (where it sits in the sequence). Together, these gave BERT the ability to capture complex semantic relationships at both the sentence and document level. This leap made BERT state-of-the-art for tasks like question answering and semantic search.
- Dense vectors capture deep semantics, handling synonyms and paraphrases (e.g., “iPhone launch”, ≈ “Apple releases new phone”).
- Sparse vectors assign explicit term weights. Even if a keyword doesn’t appear, the model can infer relevance—for example, linking “iPhone new product” with “Apple Inc.” and “smartphone.”
- Multi-vectors refine dense embeddings further by allowing each token to contribute its own interaction score, which is helpful for fine-grained retrieval.
- With the right tweaks, LLMs can generate embeddings that rival, and sometimes surpass, purpose-built models. Two notable examples are LLM2Vec and NV-Embed.
- Screen with MTEB subsets. Use benchmarks, especially retrieval tasks, to build an initial shortlist of candidates.
Test with real business data. Create evaluation sets from your own documents to measure recall, precision, and latency under real-world conditions.
Check database compatibility. Sparse vectors require inverted index support, while high-dimensional dense vectors demand more storage and computation. Ensure your vector database can accommodate your choice.
Handle long documents smartly. Utilize segmentation strategies, such as sliding windows, for efficiency, and pair them with large context window models to preserve meaning.

##### [The Ultimate Guide to LLM Evaluation: Metrics, Methods & Best Practices by Kelsey Kinzer](https://www.comet.com/site/blog/llm-evaluation-guide/)