---
pubDate: 2025-03-23
---

##### [Reinventing notebooks as reusable Python programs by akshay, dylan, myles](https://marimo.io/blog/python-not-json)
##### [Highlights from Git 2.49 by Taylor Blau](https://github.blog/open-source/git/highlights-from-git-2-49/)
##### [Preview: Amazon S3 Tables in DuckDB by Sam Ansmink, Tom Ebergen, Gabor Szarnyas](https://duckdb.org/2025/03/14/preview-amazon-s3-tables.html)
##### [In S3 simplicity is table stakes by Dr Werner Vogels - https://www.allthingsdistributed.com/](https://www.allthingsdistributed.com/2025/03/in-s3-simplicity-is-table-stakes.html?utm_campaign=inbound&utm_source=feedly)

- As the team started to look at scaling, they created a test account with an enormous number of buckets and started to test rendering times in the AWS Console â€” and in several places, rendering the list of S3 buckets could take tens of minutes to complete

##### [fleetwood.dev](https://fleetwood.dev/posts/domain-specific-architectures)

- Reading and writing from memory is extraordinarily slow when compared to computation
- Using multiple different approaches, we've derived a (non-exhaustive) list of design decisions
that should hold for any AI inference accelerator, namely:

Hardware support for low precision data types
Design for asynchronous transfers from day 1
Dedicated hardware for tensor aware memory transfers
Replace your cache hierarchy with an outsized scratchpad for AI inference
For a single accelerator, turn the memory bandwidth up to 11
Design for scale-out from day 1
Dedicated communication hardware should complement compute hardware

##### [Performance of the Python 3.14 tail-call interpreter by Nelson Elhage](https://blog.nelhage.com/post/cpython-tail-call/)