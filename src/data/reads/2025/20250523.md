---
pubDate: 2025-05-23
---

##### [Announcing DuckDB 1.3.0 by The DuckDB team](https://duckdb.org/2025/05/21/announcing-duckdb-130)
##### [How to Evaluate AI that's Smarter than Us - ACM Queue by Chip Huyen](https://queue.acm.org/detail.cfm?id=3722043)

- Functional correctness – evaluating AI by how well it accomplishes its intended tasks.
- AI-as-a-judge – using AI instead of human experts to evaluate AI outputs.
- Comparative evaluation – evaluating AI systems in relationship with each other instead of independently.
- MMLU (Massive Multitask Language Understanding
- For some applications, figuring out evaluation can take up the majority of the development effort.
- Because evaluation is difficult, many people settle for word of mouth (e.g., someone says that model X is good) or eyeballing the results (also known as vibe check). This creates even more risk and slows down application iteration. Instead, an investment in systematic evaluation is needed to make the results more reliable
- your task can be evaluated by functional correctness, that's what you should do
- Limitations of AI-as-a-judge
- Despite the many advantages of AI-as-a-judge, some teams are hesitant to adopt this approach. Using AI to evaluate AI seems tautological. The probabilistic nature of AI makes it seem too unreliable to act as an evaluator. AI judges can potentially introduce nontrivial costs and latency to an application. Given these limitations, some teams see AI-as-a-judge as a fallback option when they don't have any other way of evaluating their systems
- One big question is whether the AI judge needs to be stronger than the model being evaluated
- Using a model to judge itself—self-evaluation or self-critique—sounds like cheating, especially because of self-bias. Self-evaluation can be great for sanity checks, however. If a model thinks its response is incorrect, the model might not be that reliable. Beyond sanity checks, asking a model to evaluate itself can nudge the model to revise and improve its responses
- With comparative evaluation, you evaluate models against each other and compute a ranking from comparison results. For responses whose quality is subjective, comparative evaluation is typically easier to do than pointwise evaluation

##### [Reducing Runtime Errors in Spark: Why We Migrated from DataFrame to Dataset by Agoda Engineering](https://medium.com/agoda-engineering/reducing-runtime-errors-in-spark-why-we-migrated-from-dataframe-to-dataset-5b8fc5ac7297)

- Off-heap memory refers to memory that is managed outside the JVM heap. It is directly allocated and managed by Spark, bypassing the JVM’s garbage collection (GC) mechanism
- Tungsten avoids creating individual JVM objects for each row or column. Instead, it uses a binary format to represent data in memory.For example, instead of creating an object for each record, Spark stores the data as a contiguous block of memory in a serialized format, which is faster to process.
- How Dataset Reduces Runtime Errors
- Dataset provides type safety at compile time, while DataFrame does not.
- No Hard-Coded Column Names
- Schema Awareness and Readability
- Encoders for Optimized Serialization/Deserialization
- Encoders generate schema-specific bytecode, which is then compiled into JVM bytecode. This bytecode is highly optimized for execution.The JVM’s Just-In-Time (JIT) compiler dynamically compiles frequently executed code paths (hot code) into native machine code at runtime, further improving performance.
