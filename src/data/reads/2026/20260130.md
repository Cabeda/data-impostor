---
pubDate: 2026-01-30
---

##### [Google DeepMind Introduces ATLAS Scaling Laws for Multilingual Language Models by Robert Krzaczy≈Ñski](https://www.infoq.com/news/2026/01/google-deepmind-atlas/)

- Google DeepMind researchers have introduced ATLAS, a set of scaling laws for multilingual language models that formalize how model size, training data volume, and language mixtures interact as the number of supported languages increases.
- Results show that fine-tuning is more compute-efficient at lower token budgets, while pre-training becomes advantageous once training data and compute exceed a language-dependent threshold. For 2B-parameter models, this crossover typically occurs between about 144B and 283B tokens, providing a practical guideline for selecting an approach based on available resources
- Rather than an enormous model that is trained on redundant data from every language, how large would a purely translation model need to be, and how much smaller would it make the base model?

##### [Google Introduces TranslateGemma Open Models for Multilingual Translation by Daniel Dominguez](https://www.infoq.com/news/2026/01/google-translategemma-models/)
##### [Why DuckDB is my first choice for data processing](https://www.robinlinacre.com/recommend_duckdb/)
##### [How to parametrize exception testing in PyTest? by Kacper Borucki](https://borutzki.github.io/2026/01/15/how-to-parametrize-exception-testing-in-pytest.html)